---
title: "Crime Analysis: Kronos Incident"
description: |
  Using R to complete the text analysis, geographic analysis and visualizaiton required by IEEE VAST 2021 Challenge: mini challenge 3
author:
  - name: LI Zhen
    url: https://www.linkedin.com/in/zhen-li-8135b5189/
    affiliation: School of Computing and Information System, Singapore Management University
    affiliation_url: https://scis.smu.edu.sg/
date: 07-18-2021
preview: pic/MC3.jpg
output:
  distill::distill_article:
    toc: true
    toc_depth: 2
    self_contained: false
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,fig.retina=3)
```
# 1. Introduction

The [2021 IEEE Visual Analytics Science and Technology (VAST) Challenge](https://vast-challenge.github.io/2021/MC3.html) brings back a classic challenge of 2014 with different questions. This is about a crime incident which is a kidnapping GAStech's employees.

In the last 20 years, GAStech has brought remarkable profits to the island country of Kronos by product natural gas. Therefore, they also built strong relationships with the government of Kronos. Although GAStech was very successful, their business model hurt environment of island hugely and arouse dissatisfaction among those who advocate environmental protection.

In January, 2014, the leaders of GAStech are celebrating their new-found fortune as a result of the initial public offering of their very successful company. In the midst of this celebration, several employees of GAStech go missing. An organization known as the Protectors of Kronos (POK) is suspected in the disappearance, but things may not be what they seem.

We will use visualization tool, R, to help analyze the crime and answer the questions below:

![](pic/2021q.jpg){width=800}

# 2. Literature Review 

Since the VAST challenge 2014 had the same situation, the literature review will be conducted based on the research before. However, the questions are different between 2021 and 2014. The questions of 2014 are as following:

![](pic/2014q.jpg)

For the solutions provided by [Middlesex University](https://www.cs.umd.edu/hcil/varepository/VAST%20Challenge%202014/challenges/MC3%20-%20Real-Time,%20Streaming%20Social%20Media/entries/Middlesex%20University/), they listed the events that need further investigation and the amount of messages during the 3 hours. They also build the geographic image showing the location of events based on the Ablia map. They chose the 'Apartment Fire' as the event which most likely to provide additional clues to investigation. Then they listed all the messages relevant to apartment fire and build the map to show locations of fire. Although the list of messages can make readers know the content clearly, the visualization images are quite singular and lack the interactive part to show the relationships. Moreover, their analysis are subjective by marking out the negative, positive and neutral messages without state the criteria of judgment.

![](pic/li1.jpg)

![](pic/li2.jpg)

For the solutions provided by [Tianjin University](https://www.cs.umd.edu/hcil/varepository/VAST%20Challenge%202014/challenges/MC3%20-%20Real-Time,%20Streaming%20Social%20Media/entries/Tianjin%20University/), they mainly used the word cloud and variation charts of different words. They also build the map based on the messages. The corresponding colors between the word cloud and variation chart are difficult for readers to recognize. 

![](pic/li3.jpg)

# 3. Data Preparation

## 3.1 R packages setting

This code chunk checks whether the required packages were installed and run the packages. If the packages were not installed before, the next code line will install these packages and run them.

```{r}
packages = c('raster','sf','tmap','clock','tidyverse','ggplot2','plotly',
             'tidytext','topicmodels','quanteda','stm','dplyr','rmarkdown'
             ,'wordcloud','scales','widyr','igraph','ggraph','rgdal','gifski')
for (p in packages){
  if(!require(p,character.only = T)){
    install.packages(p)
  }
  library(p,character.only =  TRUE)
}

```

## 3.2 Appending the files 

There are three .csv files about the messages during the three hours on 23 Apr 2014 evening. First we need to merge and append the files. From the information of columns, we can find that the data type of date is wrong. Therefore, we need to change the data type of date into date time.

```{r}
m1 <- read_csv("data/csv-1700-1830.csv")
m2 <- read_csv("data/csv-1831-2000.csv")
m3 <- read_csv("data/csv-2001-2131.csv")
mall <- rbind(m1,m2,m3)
write.csv(mall,"data/mall.csv")
mall$`date(yyyyMMddHHmmss)`<- date_time_parse(as.character(mall$`date(yyyyMMddHHmmss)`),
                                              zone = "",format = "%Y%m%d %H%M%S")
glimpse(mall)
```

Then we need to filter the data based on the type column into "mbdata" and "ccdata".

```{r}
mbdata <- mall%>%
  filter(type == "mbdata")
glimpse(mbdata)
ccdata <- mall%>%
  filter(type == "ccdata")
glimpse(ccdata)
```


# 4. Answers for Mini Challenge 3 

## 4.1 Distinguish meaningful messages

The first question is as following:

Using visual analytics, characterize the different types of content in the data set. What distinguishes meaningful event reports from typical chatter from junk or spam? Please limit your answer to 8 images and 500 words.

To answer this question, we need to first identify what are the meaningful messages. Since the purpose of visualization is to control the risky events, we decided to consider the meaningful messages as the messages that can contribute to find the risk or vicious events. Based on our observation, there are some authors are the official one. Therefore, we can use the top useful words from official messages to filter the useful messages from all messages.

```{r}
# extract data from mbdata
text_mb1 <- tibble(line = 1:3872,Time = mbdata$`date(yyyyMMddHHmmss)`, 
                   author = mbdata$author, text = mbdata$message)
text_mb1<-text_mb1 %>%
  separate(text, into = c("text", "tag"), sep = "#")%>%
  select(-tag)
text_mb1
```

```{r}
# convert the text messages from sentences to words
text_mb <- text_mb1 %>%
  unnest_tokens(word,text)
```

```{r}
#To find the useful hot words from official channels messages, 
#we need to extract the words from them and then clean the words. 
useful_author <- text_mb %>%
  subset(text_mb$author == "POK" |text_mb$author == "AbilaFireDept"
         |text_mb$author == "AbilaPoliceDepartment")

useful_author1 <- useful_author %>%
  filter(!word %in% stop_words$word)
```

```{r}
count_use <-  useful_author1 %>%
  group_by(author)%>%
  count(word)%>%
  top_n(10)
paged_table(count_use)
```

```{r}
#Extract the rest data.
evaluate <- text_mb%>%
  subset(text_mb$author != "POK" &text_mb$author != "AbilaFireDept"
         &text_mb$author != "AbilaPoliceDepartment")
```

```{r}
#Extract the messages contains the key words and extract them.
evaluate_use <- evaluate %>%
  group_by(line)%>%
  inner_join(count_use,by = "word")%>%
  select(line:word)%>%
  inner_join(text_mb, by ="line")%>%
  select(-Time.y,-author.x,-word.x)
evaluate_use <- unique(evaluate_use)

evaluate_nuse <- evaluate %>%
  group_by(line)%>%
  anti_join(evaluate_use,by = "line")
paged_table(evaluate_nuse)

evaluate_use <- text_mb %>%
  group_by(line)%>%
  anti_join(evaluate_nuse,by = "line")
paged_table(evaluate_use)
```

Therefore, we have the dataset ***evaluate_use*** for meaningful messages and ***evaluate_nuse*** for junk or spam messages. We will build the charts based on these datasets. 

```{r}
# top10 hot words comparasion between meaningful and junk messages
evaluate_use1 <- evaluate_use %>%
  subset(!word %in% stop_words$word)
         
evaluate_nuse1 <- evaluate_nuse %>%
  filter(!word %in% stop_words$word)
count_evaluate_use <- evaluate_use1 %>%
  ungroup()%>%
  count(word)%>%
  top_n(10)%>%
  arrange(desc(n))
count_evaluate_nuse <- evaluate_nuse1 %>%
  ungroup()%>%
  count(word)%>%
  top_n(10)%>%
  arrange(desc(n))
```

To ensure the quality of hot words to be useful for comparison. We first remove the meaningless words from datasets. In the code chunks below, we built the bar chart and word cloud for hot words.

```{r count_evaluate_use, fig.align='center',fig.width=8,fig.height=5}
plot_ly(data = count_evaluate_use,
        x = ~n,
        y = ~reorder(word,n),
        text = ~n,
        marker = list(color = 'rgb(158,202,225)',
                           line = list(color = 'rgb(8,48,107)', width = 1.5)),
        textposition = 'outside',
        type = "bar")%>%
  layout(title = 'Top 10 hot key words from meaningful messages',
         xaxis=list(title = ""),
         yaxis = list(title = ""),
         categorxorder = "value")
```

```{r evaluate_use1, fig.align='center',fig.width=8,fig.height=5}
evaluate_use1 %>%
  ungroup()%>%
  count(word)%>%
  with(wordcloud(word, n, max.words = 100))
```

```{r count_evaluate_nuse, fig.align='center',fig.width=10,fig.height=5}
plot_ly(data = count_evaluate_nuse,
        x = ~n,
        y = ~reorder(word,n),
        text = ~n,
        marker = list(color = 'rgb(158,202,225)',
                           line = list(color = 'rgb(8,48,107)', width = 1.5)),
        textposition = 'outside',
        type = "bar")%>%
  layout(title = 'Top 10 hot key words from junk messages',
         xaxis=list(title = ""),
         yaxis = list(title = ""),
         categorxorder = "value")

evaluate_nuse1 %>%
  ungroup()%>%
  count(word)%>%
  with(wordcloud(word, n, max.words = 100))
```

From the charts, we can see that count of "pokrally" from junk messages is the highest and count of "ablia" is the highest from meaningful messages. Apart from the single word dimension, we can also explore the relationship between the co occur words.

```{r}
#co-occur words for meaningful messages
pairs_use <- evaluate_use1 %>% 
  ungroup()%>%
  pairwise_count(word, line, sort = TRUE)
paged_table(pairs_use)

set.seed(1234)
pairs_use %>%
  filter(n >= 30) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()+
  ggtitle("Co-occur words for meaningful messages")
```

```{r}
#co-occur words for junk messages
pairs_nuse <- evaluate_nuse1 %>% 
  ungroup()%>%
  pairwise_count(word, line, sort = TRUE)
paged_table(pairs_nuse)

set.seed(1234)
pairs_nuse %>%
  filter(n >= 20) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "red") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()+
  ggtitle("Co-occur words for junk messages")
```

From the word dimension, we can find that it is more frequently for words appear together in meaningful message than those appear in junk messages. This indicate that meaningful messages have more converge topics. Therefore, there were more fixed words groups in meaningful messages.

```{r}
nuse <- evaluate_nuse1%>%
  select(line)
nuse = unique(nuse)
nuse$useful = "junk"
dif_message <- text_mb1%>%
  left_join(nuse,by = "line")
dif_message <- dif_message %>%
  replace_na(list(useful = "meaningful"))
paged_table(dif_message)
```

```{r dif_message,fig.align='center',fig.width=10,fig.height=5}

time<-ggplot(data = dif_message,aes(x = Time,fill=useful,color = useful))+
  geom_histogram(alpha = 0.5,position="identity",bins = 50)+
  facet_wrap(~useful, ncol = 1)+
  ggtitle('Distribution of frequency of sending messages from 17:00 to 21:30')+
  theme(axis.title.y =element_blank(),
        axis.title.x =element_blank())
  
time <- ggplotly(time)
  
time
```

From the chart, we can find that meaningful messages have two peaks which happened around the 18:50 pm and 19:45 pm. For junk messages, there were only one peak which happened overlap with the latter peak of meaningful messages.

```{r}
frequency <- dif_message %>% 
  group_by(author) %>% 
  count(useful, sort = TRUE) %>% 
  left_join(dif_message %>% 
              group_by(author) %>% 
              summarise(total = n())) %>%
  mutate(freq = n/total)
frequency
```

```{r}
frequency <- frequency %>% 
  select(author, useful, freq) %>% 
  pivot_wider(names_from = useful, values_from = freq) %>%
  arrange(junk, meaningful)
frequency$junk = as.numeric(frequency$junk)
frequency$meaningful = as.numeric(frequency$meaningful)
frequency <- frequency%>%
  replace_na(list(junk = 0))%>%
  replace_na(list(meaningful = 0))
```

```{r frequency, fig.align='center',fig.width=10,fig.height=5}
scatter <- ggplot(frequency, aes(junk, meaningful,)) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  geom_abline(color = "red")+
  ggtitle("Comparing the frequency of junk and meaningful messages")
ggplotly(scatter)
```

From this chart, we can find that many authors sent both junk messages and meaningful messages because there are more points in the middle of charts. However, when we compare which authors send more junk messages or meaningful messages, it is easy to find that authors send more junk messages and there were higher number of authors who have 90% of messages are junk messages.

## 4.2 Risk evaluation

The second question is as following:

Use visual analytics to represent and evaluate how the level of the risk to the public evolves over the course of the evening. Consider the potential consequences of the situation and the number of people who could be affected. Please limit your answer to 10 images and 1000 words.

To answer this question, we need to first address the risk events happened during the 17:00 to 21:30 period. To ensure the authenticity of the event, we need to use ccdata and mbdata to track the accidents. With ccdata, we can find the accidents and use the related messages from mbdata to get more details of accidents.

```{r}
clean_cc <- tibble(id =1:191,time = ccdata$`date(yyyyMMddHHmmss)`,
                   messages = ccdata$message, locations = ccdata$location)
clean_cc <- clean_cc%>%
  filter(messages!="INCOMPLETE CALL FOR POLICE")
clean_cc$messages = tolower(clean_cc$messages)
paged_table(clean_cc)
```

To better evaluate the level of risk, we should first set the standard for risks. Therefore, we have developed a measurement range of 0-5 for the level of risk. 0 represents the situation when police check and found no risks. 1 refers to situations with the lowest risk, such as traffic jams. 5 represents the most risky situation, such as fire or death. We will first look at ccdata to find the accidents and evaluate the sensitivity based on details provided in mbdata.

```{r}
event_unqiue <- unique(clean_cc$messages)
```

From the event_unique, we can find that there are some accidents' risk level can be predefined. By doing this, we can focus on more server and complicated events. By reading through the ccdata, we can find that there are 4 server events happened during the 17:00 to 21:30. They are fire in dancing dolphin, hit and run accident and shoot and standoff. These are the most severe events and we set the risk level of related messages to be 5. One left is gathering in the Ablia city park and we set this as 4.

```{r}
risk_cc <- clean_cc
low <- c("traffic stop", "business check", "subject stop","disturbance-noise","prowler")
severe <- c("vehicle accident-report",
            "fire/medics needed-fire trucks #41 and #57 dispatched to fire",
            "felony hit & run - in progress-with injury",
            "police needed-police units #101 and #220 dispatched to fire",
            "fire/medics needed-amublance dispatched to fire",
            "fire/medics needed-fire truck #23 dispatched to fire",
            "felony hit & run-in progress-with injury",
            "all units broadcast-felony hit & run-black van/partial plate #l829",
            "fire/medics dispatched-pedestrian hit & run",
            "fire/medics needed-officer down",
            "fire/medics needed-ambulance dispatched",
            "dire emergency/police needed-officer down-send additional units",
            "dire emergency/swat team dispatched",
            "pursuit continues-police units #253 and #357/second unit joins",
            "pursuit-continues-police units #253 and #357",
            "pursuit-police unit #253 in pursuit",
            "all units broadcast-building fire"
            )
middle <- c("keep the peace-crowd control/abila city park","park check",
            "crowd control-street closure","possible fire-report",
            "police unit dispatched-crowd control")
risk_cc$risk <- ifelse(risk_cc$messages %in% low, 1, 
                       ifelse(risk_cc$messages == "alarm-secure no crime",0,
                       ifelse(risk_cc$messages == "shoplifting",2,
                              ifelse(risk_cc$messages %in% severe,5,
                                     ifelse(risk_cc$messages %in% middle,4,3)))))
```

We built a chart to show the risk level changing based on time.

```{r}
plot_ly(data = risk_cc,
       x = ~time,
       y = ~risk,
       text = ~messages,
       type = "scatter",
       mode = "line"
       )%>%
  layout(title = 'Changing of risk level from 17:00 to 21:30',
         xaxis = list(title = ""))
 
```

For potential consequence and number of affected people, we will mainly focus on the four major events.

### a). Crowd gathering in City Park

First event was a crowd gathering in the Ablia City Park. To get more information about this event, we should select the messages about "park" from mbdata. Since there were a shooting happened in Gelatogalore near city park at around 19:40. To better evaluate the potential results and number of affected people, we separate them and consider the crowd gathering first.

```{r}
park <- text_mb1 %>%
  filter(grepl('park|Park|gathering',text))%>%
  filter(Time< "2014-01-23 19:40:00")%>%
  unnest_tokens(word,text)%>%
  filter(!word %in% stop_words$word)
```

```{r}
park %>%
  filter(word != 'rt'&word != 'park')%>%
  count(word)%>%
  top_n(10)%>%
  plot_ly(
        x = ~n,
        y = ~reorder(word,n),
        text = ~n,
        marker = list(color = 'rgb(158,202,225)',
                           line = list(color = 'rgb(8,48,107)', width = 1.5)),
        textposition = 'outside',
        type = "bar")%>%
  layout(title = 'Top 10 hot key words from messages related to park',
         xaxis=list(title = ""),
         yaxis = list(title = ""),
         categorxorder = "value")
```

```{r}
park %>%
  filter(word != 'rt'&word != 'park')%>%
  count(word)%>%
  with(wordcloud(word,n,max.words = 300))
```

```{r}
risk_cc %>%
  filter(grepl('Egeou',locations))%>%
  filter(time< "2014-01-23 19:20:00")
```
From the messages from mbdata and ccdata, we can all find that there were not severe hurt or injury happened during the crowd gathering. In the ccdata, we extract the record of related location and found the police put in a lot of police force to maintain the order of the park gathering. In the park, there were many POK leaders doing speech and tried to call on everyone against governance. Therefore, the potential consequence is may causing crowd protest marches or more serious riots.
To find out the number of affected people, we can filter the word which contains numbers.

```{r}
park %>%
  group_by(line)%>%
  filter(grepl("^[0-9]{1,}$", word))%>%
  paged_table()
```
```{r}
text_mb1 %>%
  filter(line == 358|line == 409)%>%
  paged_table()

```

From the messages, we can find that the number of affected people is 1200.

### b). Fire in the dancing dolphin

To first understand more details about this accidents, we need to filter the messages related to this event. Therefore, we set "fire", "dancing" and "dolphin" as the key words and also need to refer to ccdata and messages from Abila fire department. 

```{r}
firedep <- text_mb1%>%
  filter(author == "AbilaFireDept")
paged_table(firedep)
```

```{r}
firecc<- risk_cc%>%
  filter(grepl("fire",messages))
paged_table(firecc)
```

From the offical messages, we can find that the fire in dancing dolphin started from 18:40 *(ccdata - "possible fire-report")* to 21:30 *(AbilaFireDept - "Abila Fire Department reports an explosion at the Dancing Dolphin fire at Achilleos & Madeg ")*. The duration of this fire is very long and according to the records, there were an explosion at 21:30. Therefore, the risk level of it is very high.

```{r}
mbdata$message = tolower(mbdata$message)
f <- c("gun","shot")
firemb <- mbdata%>%
  filter(grepl("dancing|dolphin|fire",message))%>%
  filter(!grepl("shot|gun",message))
paged_table(firemb)
```

From the messages of fire department, witnesses saw smoke from windows of the second floor of dancing dolphin building. Abila Fire Department dsent two fire trucks and an ambulance to the apartment.Some residents successfully exited the building and were treated for smoke inhalation and minor injuries. Firemen found one resident trapped on an upper floor and later managed to rescue him. Unfortunately, one firefighter was injured during the fire and was taken to the hospital for treatment. As time went by, the fire did not seem to be under control and the top floors of Dancing Dolphin Apartments collapsed at around 20:33. Abila Fire Department sent three more fire trucks to help and the fire was under control around 21:23. At 21:30, an explosion occurred inside the Dancing Dolphin Apartments and several people were injured according to the mbdata. 

```{r}
firemb%>%
  unnest_tokens(word,message)%>%
  select(author,word,`date(yyyyMMddHHmmss)`)%>%
  filter(!word %in% stop_words$word)%>%
  filter(word != 'rt'&word != 'fire'&word != 'dancing'&word != 'dolphin'&word != 'centralbulletin')%>%
  count(word)%>%
  top_n(10)%>%
  plot_ly(
        x = ~n,
        y = ~reorder(word,n),
        text = ~n,
        marker = list(color = 'rgb(158,202,225)',
                           line = list(color = 'rgb(8,48,107)', width = 1.5)),
        textposition = 'outside',
        type = "bar")%>%
  layout(title = 'Top 10 hot key words from messages related to dancing dolphin',
         xaxis=list(title = ""),
         yaxis = list(title = ""),
         categorxorder = "value")
```

```{r}
firemb %>%
  unnest_tokens(word,message)%>%
  select(author,word,`date(yyyyMMddHHmmss)`)%>%
  filter(!word %in% stop_words$word)%>%
  filter(word != 'rt'&word != 'fire'&word != 'dancing'&word != 'dolphin'&word != 'centralbulletin')%>%
  count(word)%>%
  with(wordcloud(word,n,max.words = 300))
```

From the visualization above, we can find that there are many messages about Abila Fire Department sent the fire units to the scene and how police and firefighter respond to fire. Although the official department respond quickly, there were an explosion happened. Therefore, the potential consequence of situation could be residents in and around apartment may get injured or even dead. Moreover, the firefighters could also be injured or even dead because of fire.

```{r}
firemb %>%
  filter(grepl("^[0-9]{1,}$", message))%>%
  paged_table()
```

Since there is no messages about the exact number residents being affected, it is hard to estimate the number. Based on what we have now, the affected people are mainly the residents of dancing dolphin department and people around the dancing dolphin when explosion happened. The dancing dolphin aprtment has six floors. Therefore, we estimate the number of affected people could be around 100.

### c). Black van

To get more information of black van events, we need to combine the messages from Ablia Police Department and mbdata together.

```{r}
black_police <- mbdata %>%
  filter(author == "AbilaPoliceDepartment")%>%
  select(-location,-latitude,-longitude,-type)
paged_table(black_police)
```

From the messages of Abila Police Department, we can have a review of event process. At 19:19, a black van hit a car on Souliou St and escaped. Fortunately, no or unknown injuries were reported. Later, the van struck a bicyclist on the 500 block of Schaber St. Luckily, the bicyclist was uninjured according to the mbdata. Abila Police Department broadcasted the accident and released a partial license plate number of the vehicle. The black van went West on Egeou St and was pursued by police. After getting trapped by police in the Gelato Galore parking lot, at the corner of Ithakis St and Alexandrias St, the suspects opened the doors and fired at police officers at 19:40. A police officer was shot. An ambulance arrived on the scene and rescued the wounded officer. Luckily, they reported this officer were stable. More police arrived and evacuated the corner of Ithakis St and Alexandrias St for public safety. Then, the Abila Police SWAT team arrived to help handle the standoff. The gunmen revealed they were holding hostages in the van and threatened to kill them if the police didn't let the gunmen go. At 20:11, the SWAT team began to negotiate with the shooter. The negotiation took more than half an hour and at 21:00, one gunman went back in the van. Witnesses saw two suspects were fighting in the van. At around 21:20, two suspects surrendered and two hostages were rescued. 

```{r}
black_mb <- mbdata%>%
  filter(grepl("black|van|shot|hostage|hit|standoff",message))%>%
  filter(`date(yyyyMMddHHmmss)`> "2014-01-23 19:15:00")
paged_table(black_mb)
```

```{r}
black_word <- black_mb %>%
  select(-type,-latitude,-longitude,-location)%>%
  unnest_tokens(word,message)%>%
  filter(!word %in% stop_words$word)
```

```{r}
black_word%>%
  filter(word != "rt")%>%
  count(word)%>%
  top_n(10)%>%
  plot_ly(
        x = ~n,
        y = ~reorder(word,n),
        text = ~n,
        marker = list(color = 'rgb(158,202,225)',
                           line = list(color = 'rgb(8,48,107)', width = 1.5)),
        textposition = 'outside',
        type = "bar")%>%
  layout(title = 'Top 10 hot key words from messages related to black van',
         xaxis=list(title = ""),
         yaxis = list(title = ""),
         categorxorder = "value")
```

```{r}
black_word%>%
  count(word)%>%
  filter(word != "rt")%>%
  with(wordcloud(word,n,max.words = 300))
```

From the messages, we can conclude that the potential consequence based on black van events can be very severe. It is very lucky that the kidnapping ends with a peaceful result. However, the worst potential consequence is the death of four or even more innocent people.

```{r}
num_black <- black_word %>%
  filter(grepl("^[0-9]{1,}$",word))
unique(num_black$word)
```
```{r}
black_mb %>%
  filter(grepl("500|100|3|200",message))%>%
  paged_table()

```

From the messages selected based on numbers, we can find that there are 200 people gathered to watch standoff *(close to 200 people have gathered to watch standoff at alexandrias & ithakis #abilapost)*. Therefore, the number of affected people is around 200. 

## 4.3 Event respondence

The third question is as following:

If you were able to send a team of first responders to any single place, where would it be? Provide your rationale. How might your response be different if you had to respond to the events in real time rather than retrospectively? Please limit your answer to 8 images and 500 words.

For the first question, my answer is **dancing dolphin apartment**. Since the potential consequences of fire in dancing dolphin apartment and black van are all severe, the locations of these two incidents are close. It will be easy for police department to control the severe incidents. 

```{r}
bgmap <- raster("D:/second_term/Visual Analytics/Assignment/MC3/Geospatial/MC2-tourist.tif")
tm_shape(bgmap) +
tm_rgb(bgmap, r = 1,g = 2,b = 3,
       alpha = NA,
       saturation = 1,
       interpolate = TRUE,
       max.value = 255)
```
```{r}
Abila_st <- st_read(dsn = "D:/second_term/Visual Analytics/Assignment/MC3/Geospatial",
                    layer = "Abila")
```
```{r}
gps <- mbdata %>%
  select(`date(yyyyMMddHHmmss)`,message,latitude,longitude)%>%
  drop_na()

gps_sf <- st_as_sf(gps, 
                   coords = c("longitude", "latitude"),
                       crs= 4326)

gps_point <- gps_sf %>%
  group_by(message) %>%
  summarize(m = mean(`date(yyyyMMddHHmmss)`),
            do_union = FALSE)%>%
  st_cast("MULTIPOINT")
```

```{r}
tmap_mode("view")
tm_shape(bgmap) +
  tm_rgb(bgmap, r = 1,g = 2,b = 3,
       alpha = NA,
       saturation = 1,
       interpolate = TRUE,
       max.value = 255) +
  tm_shape(gps_point) +
  tm_dots(col = "red")
```

The city park is far away from dancing dolphin and black van vehicle route. Moreover, the potential consequence is not as severe as fire and kidnapping. POK tried to use this gathering to distract the police's attention. 

However, if i had to respond the event in real time, i will send police to city park first to prevent the potential protest march and fights among people. 











